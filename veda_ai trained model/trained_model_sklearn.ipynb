{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('edited_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter No</th>\n",
       "      <th>Verse No</th>\n",
       "      <th>Shloka</th>\n",
       "      <th>English Translation</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>धृतराष्ट्र उवाच | धर्मक्षेत्रे कुरुक्षेत्रे सम...</td>\n",
       "      <td>Dhritarastra said: O Sanjaya, what did my sons...</td>\n",
       "      <td>The two armies had gathered on the battlefield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>सञ्जय उवाच । दृष्ट्वा तु पाण्डवानीकं व्यूढं दु...</td>\n",
       "      <td>Sanjaya said: But then, seeing the army of the...</td>\n",
       "      <td>Sanjay understood Dhritarashtra’s concern, who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् । व...</td>\n",
       "      <td>O teacher, (please) see this vast army of the ...</td>\n",
       "      <td>Duryodhana asked Dronacharya to look at the sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>अत्र शूरा महेष्वासा भीमार्जुनसमा युधि | युयुधा...</td>\n",
       "      <td>There are in this army, heroes wielding great ...</td>\n",
       "      <td>Due to his anxiety, the Pandava army seemed mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>धृष्टकेतुश्चेकितान: काशिराजश्च वीर्यवान् | पुर...</td>\n",
       "      <td>Dhrstaketu, Cekitana, and the valiant king of ...</td>\n",
       "      <td>Due to his anxiety, the Pandava army seemed mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chapter No  Verse No                                             Shloka  \\\n",
       "0           1         1  धृतराष्ट्र उवाच | धर्मक्षेत्रे कुरुक्षेत्रे सम...   \n",
       "1           1         2  सञ्जय उवाच । दृष्ट्वा तु पाण्डवानीकं व्यूढं दु...   \n",
       "2           1         3  पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् । व...   \n",
       "3           1         4  अत्र शूरा महेष्वासा भीमार्जुनसमा युधि | युयुधा...   \n",
       "4           1         5  धृष्टकेतुश्चेकितान: काशिराजश्च वीर्यवान् | पुर...   \n",
       "\n",
       "                                 English Translation  \\\n",
       "0  Dhritarastra said: O Sanjaya, what did my sons...   \n",
       "1  Sanjaya said: But then, seeing the army of the...   \n",
       "2  O teacher, (please) see this vast army of the ...   \n",
       "3  There are in this army, heroes wielding great ...   \n",
       "4  Dhrstaketu, Cekitana, and the valiant king of ...   \n",
       "\n",
       "                                         Explanation  \n",
       "0  The two armies had gathered on the battlefield...  \n",
       "1  Sanjay understood Dhritarashtra’s concern, who...  \n",
       "2  Duryodhana asked Dronacharya to look at the sk...  \n",
       "3  Due to his anxiety, the Pandava army seemed mu...  \n",
       "4  Due to his anxiety, the Pandava army seemed mu...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700 entries, 0 to 699\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Chapter No           700 non-null    int64 \n",
      " 1   Verse No             700 non-null    int64 \n",
      " 2   Shloka               700 non-null    object\n",
      " 3   English Translation  700 non-null    object\n",
      " 4   Explanation          700 non-null    object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 27.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(verse.split()) for verse in df['Shloka']])\n",
    "num_classes = df['Chapter No'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Shloka']\n",
    "y = pd.get_dummies(df['Chapter No']).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 6s 162ms/step - loss: 2.8722 - accuracy: 0.0839 - val_loss: 2.8310 - val_accuracy: 0.1286\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 2.8421 - accuracy: 0.1071 - val_loss: 2.8094 - val_accuracy: 0.1286\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.8205 - accuracy: 0.1107 - val_loss: 2.8231 - val_accuracy: 0.1286\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.8094 - accuracy: 0.1071 - val_loss: 2.8175 - val_accuracy: 0.1286\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.7495 - accuracy: 0.1107 - val_loss: 2.7726 - val_accuracy: 0.1143\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3912 - accuracy: 0.1571 - val_loss: 3.2730 - val_accuracy: 0.1143\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.3492 - accuracy: 0.1643 - val_loss: 3.4158 - val_accuracy: 0.0571\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.2566 - accuracy: 0.1893 - val_loss: 3.4841 - val_accuracy: 0.0643\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 2.2535 - accuracy: 0.1804 - val_loss: 3.5655 - val_accuracy: 0.0643\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.2956 - accuracy: 0.1804 - val_loss: 3.1884 - val_accuracy: 0.0857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14904453cd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After preprocessing the data, you can train the model using the fit() method\n",
    "model.fit(X_train_seq_padded, y_train, validation_data=(X_test_seq_padded, y_test), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 10ms/step - loss: 3.1884 - accuracy: 0.0857\n",
      "Test loss: 3.188405990600586\n",
      "Test accuracy: 0.08571428805589676\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "loss, accuracy = model.evaluate(X_test_seq_padded, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test loss is 3.188 and the test accuracy is 0.086. This means that the model is not performing very well and needs improvement. The accuracy of 0.086 indicates that the model is correctly predicting the chapter number of only about 8.6% of the verses in the test set. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for model's better perfomance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('edited_data.csv')\n",
    "max_length = max([len(verse.split()) for verse in df['Shloka']])\n",
    "num_classes = df['Chapter No'].nunique()\n",
    "X = df['Shloka']\n",
    "y = pd.get_dummies(df['Chapter No']).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "# Try different embedding dimensions\n",
    "embedding_dim = 300\n",
    "\n",
    "# Use pre-trained word embeddings such as GloVe or Word2Vec\n",
    "# embedding_matrix = create_embedding_matrix('path/to/embeddings_file', tokenizer.word_index, embedding_dim)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the embedding dimension can help the model performance in a few ways. The embedding layer is responsible for learning the relationship between words in the input text, and the embedding dimension represents the size of the vector that represents each word. A higher embedding dimension can capture more complex relationships between words, but can also increase the number of parameters in the model, making it more computationally expensive and potentially leading to overfitting.\n",
    "\n",
    "In some cases, a higher embedding dimension may lead to better performance, as it allows the model to capture more nuanced relationships between words. However, the optimal embedding dimension can vary depending on the size of the dataset, the complexity of the task, and the specific language used in the input text. Therefore, it's important to experiment with different embedding dimensions to find the one that works best for a particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more dense layers after the LSTM layer to increase model capacity\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regularization techniques such as dropout to prevent overfitting\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 [==============================] - 10s 289ms/step - loss: 2.8781 - accuracy: 0.0750 - val_loss: 2.8533 - val_accuracy: 0.0500\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 1s 157ms/step - loss: 2.8503 - accuracy: 0.0732 - val_loss: 2.8323 - val_accuracy: 0.1643\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 1s 145ms/step - loss: 2.8330 - accuracy: 0.0929 - val_loss: 2.8313 - val_accuracy: 0.1286\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 2.8025 - accuracy: 0.1161 - val_loss: 2.7884 - val_accuracy: 0.1357\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 2.5330 - accuracy: 0.1429 - val_loss: 3.1116 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 2.5129 - accuracy: 0.1286 - val_loss: 3.0574 - val_accuracy: 0.0929\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 2.5288 - accuracy: 0.1179 - val_loss: 2.9331 - val_accuracy: 0.0500\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 1s 145ms/step - loss: 2.4549 - accuracy: 0.1554 - val_loss: 2.9617 - val_accuracy: 0.1286\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 1s 137ms/step - loss: 2.3732 - accuracy: 0.1482 - val_loss: 2.8486 - val_accuracy: 0.1214\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 1s 137ms/step - loss: 2.2735 - accuracy: 0.1893 - val_loss: 2.8718 - val_accuracy: 0.1357\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 1s 148ms/step - loss: 2.1435 - accuracy: 0.2268 - val_loss: 3.1482 - val_accuracy: 0.1357\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 1s 144ms/step - loss: 1.9122 - accuracy: 0.3071 - val_loss: 3.4993 - val_accuracy: 0.1000\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 1.8367 - accuracy: 0.3089 - val_loss: 3.9232 - val_accuracy: 0.0857\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 1.6967 - accuracy: 0.3768 - val_loss: 4.1415 - val_accuracy: 0.0857\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 1s 136ms/step - loss: 1.4781 - accuracy: 0.4411 - val_loss: 4.3809 - val_accuracy: 0.0571\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 1.3908 - accuracy: 0.4875 - val_loss: 4.5490 - val_accuracy: 0.0643\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 1s 132ms/step - loss: 1.3246 - accuracy: 0.4929 - val_loss: 4.9157 - val_accuracy: 0.0357\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 1s 137ms/step - loss: 1.2065 - accuracy: 0.5482 - val_loss: 4.8394 - val_accuracy: 0.0571\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 1.0760 - accuracy: 0.5768 - val_loss: 5.4105 - val_accuracy: 0.0500\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 2s 192ms/step - loss: 1.0332 - accuracy: 0.5911 - val_loss: 5.4151 - val_accuracy: 0.0500\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 2s 192ms/step - loss: 0.9461 - accuracy: 0.6161 - val_loss: 5.6848 - val_accuracy: 0.0429\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 2s 210ms/step - loss: 0.8581 - accuracy: 0.6589 - val_loss: 5.0492 - val_accuracy: 0.0643\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 0.8117 - accuracy: 0.6964 - val_loss: 6.0275 - val_accuracy: 0.0714\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 2s 167ms/step - loss: 0.7242 - accuracy: 0.7179 - val_loss: 6.0574 - val_accuracy: 0.0786\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.7341 - accuracy: 0.7250 - val_loss: 6.3876 - val_accuracy: 0.0571\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 1s 140ms/step - loss: 0.6545 - accuracy: 0.7607 - val_loss: 7.0157 - val_accuracy: 0.0786\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.6681 - accuracy: 0.7714 - val_loss: 6.6422 - val_accuracy: 0.0571\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 1s 140ms/step - loss: 0.5662 - accuracy: 0.8071 - val_loss: 6.7417 - val_accuracy: 0.1071\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 1s 138ms/step - loss: 0.5392 - accuracy: 0.8232 - val_loss: 7.8367 - val_accuracy: 0.0714\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 0.5752 - accuracy: 0.8125 - val_loss: 6.6859 - val_accuracy: 0.0857\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 1s 138ms/step - loss: 0.4947 - accuracy: 0.8179 - val_loss: 7.5183 - val_accuracy: 0.0714\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 1s 140ms/step - loss: 0.4818 - accuracy: 0.8286 - val_loss: 7.5223 - val_accuracy: 0.0714\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 1s 137ms/step - loss: 0.5570 - accuracy: 0.7929 - val_loss: 6.8362 - val_accuracy: 0.0929\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 1s 143ms/step - loss: 0.4869 - accuracy: 0.8375 - val_loss: 7.1431 - val_accuracy: 0.1214\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 1s 140ms/step - loss: 0.4204 - accuracy: 0.8679 - val_loss: 7.3005 - val_accuracy: 0.1143\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 1s 137ms/step - loss: 0.4271 - accuracy: 0.8357 - val_loss: 7.3201 - val_accuracy: 0.1214\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 1s 144ms/step - loss: 0.4603 - accuracy: 0.8321 - val_loss: 7.5101 - val_accuracy: 0.1000\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 1s 136ms/step - loss: 0.3493 - accuracy: 0.8839 - val_loss: 7.7508 - val_accuracy: 0.0929\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.3104 - accuracy: 0.8964 - val_loss: 7.7447 - val_accuracy: 0.1286\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 1s 147ms/step - loss: 0.2844 - accuracy: 0.9018 - val_loss: 7.9858 - val_accuracy: 0.1357\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 1s 148ms/step - loss: 0.2419 - accuracy: 0.9196 - val_loss: 8.0606 - val_accuracy: 0.1214\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 1s 145ms/step - loss: 0.2221 - accuracy: 0.9161 - val_loss: 8.4277 - val_accuracy: 0.1357\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 0.2009 - accuracy: 0.9214 - val_loss: 8.6009 - val_accuracy: 0.1357\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.1908 - accuracy: 0.9232 - val_loss: 8.7422 - val_accuracy: 0.1214\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 1s 139ms/step - loss: 0.1946 - accuracy: 0.9125 - val_loss: 8.5005 - val_accuracy: 0.1286\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 1s 143ms/step - loss: 0.3041 - accuracy: 0.9036 - val_loss: 8.7170 - val_accuracy: 0.1357\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.1873 - accuracy: 0.9232 - val_loss: 8.8700 - val_accuracy: 0.1000\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.1865 - accuracy: 0.9286 - val_loss: 9.0821 - val_accuracy: 0.1286\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.1716 - accuracy: 0.9304 - val_loss: 8.8874 - val_accuracy: 0.1214\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 1s 140ms/step - loss: 0.2130 - accuracy: 0.9179 - val_loss: 9.1079 - val_accuracy: 0.1143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14914115f90>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences to the same length\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Increase the number of epochs to allow the model to learn more from the training data\n",
    "model.fit(X_train_seq_padded, y_train, validation_data=(X_test_seq_padded, y_test), epochs=50, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 41ms/step - loss: 9.1079 - accuracy: 0.1143\n",
      "Test loss: 9.107884407043457\n",
      "Test accuracy: 0.11428571492433548\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "loss, accuracy = model.evaluate(X_test_seq_padded, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting occured - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_categorical\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m KeyedVectors\n\u001b[0;32m     11\u001b[0m \u001b[39m# Load pre-trained word embeddings\u001b[39;00m\n\u001b[0;32m     12\u001b[0m word_vectors \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39mload_word2vec_format(\u001b[39m'\u001b[39m\u001b[39mpath/to/embedding/file\u001b[39m\u001b[39m'\u001b[39m, binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Bidirectional, Conv1D, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "word_vectors = KeyedVectors.load_word2vec_format('path/to/embedding/file', binary=True)\n",
    "\n",
    "df = pd.read_csv('edited_data.csv')\n",
    "\n",
    "max_length = max([len(verse.split()) for verse in df['Shloka']])\n",
    "num_classes = df['Chapter No'].nunique()\n",
    "\n",
    "X = df['Shloka']\n",
    "y = pd.get_dummies(df['Chapter No']).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 300\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_vectors.vocab:\n",
    "        embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "\n",
    "# Add multiple LSTM layers and a dropout layer\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=128))\n",
    "\n",
    "# Add dense layers with different activation functions\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model with different optimizer and learning rate\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Increase batch size and number of epochs\n",
    "model.fit(X_train_seq_padded, y_train, validation_data=(X_test_seq_padded, y_test), epochs=20, batch_size=128)\n",
    "\n",
    "# Add a bidirectional LSTM layer and a convolutional layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(units=128)))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model with different optimizer and learning rate\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Increase batch size and number of epochs\n",
    "model.fit(X_train_seq_padded, y_train, validation_data=(X_test_seq_padded, y_test), epochs=30, batch_size=256)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_seq_padded, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
