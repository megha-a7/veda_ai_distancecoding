{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file into a Pandas dataframe\n",
    "df = pd.read_csv('edited_data.csv')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 4.54kB/s]\n",
      "c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VICTUS\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<?, ?B/s] \n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:01<00:00, 227kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:02<00:00, 221kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 268M/268M [11:54<00:00, 375kB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer for the transformer model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load the pre-trained model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('edited_data.csv')\n",
    "max_length = max([len(verse.split()) for verse in df['Shloka']])\n",
    "num_classes = df['Chapter No'].nunique()\n",
    "X = df['Shloka']\n",
    "y = pd.get_dummies(df['Chapter No']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the input data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Define the model architecture\n",
    "embedding_dim = 300\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "9/9 [==============================] - 4s 140ms/step - loss: 2.8678 - accuracy: 0.0911 - val_loss: 2.8265 - val_accuracy: 0.1286\n",
      "Epoch 2/20\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 2.8305 - accuracy: 0.1071 - val_loss: 2.8287 - val_accuracy: 0.1286\n",
      "Epoch 3/20\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 2.8164 - accuracy: 0.1054 - val_loss: 2.8174 - val_accuracy: 0.1571\n",
      "Epoch 4/20\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 2.8081 - accuracy: 0.1125 - val_loss: 2.8222 - val_accuracy: 0.1286\n",
      "Epoch 5/20\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 2.7005 - accuracy: 0.1429 - val_loss: 2.7801 - val_accuracy: 0.0929\n",
      "Epoch 6/20\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 2.3550 - accuracy: 0.1464 - val_loss: 3.2770 - val_accuracy: 0.0714\n",
      "Epoch 7/20\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 2.2540 - accuracy: 0.1786 - val_loss: 3.4053 - val_accuracy: 0.0857\n",
      "Epoch 8/20\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 2.2381 - accuracy: 0.2000 - val_loss: 3.3105 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 2.1602 - accuracy: 0.2161 - val_loss: 3.3379 - val_accuracy: 0.0929\n",
      "Epoch 10/20\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 1.9503 - accuracy: 0.2750 - val_loss: 3.6578 - val_accuracy: 0.0786\n",
      "Epoch 11/20\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 1.7034 - accuracy: 0.3804 - val_loss: 4.1111 - val_accuracy: 0.0500\n",
      "Epoch 12/20\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 1.5025 - accuracy: 0.4304 - val_loss: 4.2200 - val_accuracy: 0.0857\n",
      "Epoch 13/20\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.3277 - accuracy: 0.4911 - val_loss: 4.5225 - val_accuracy: 0.0929\n",
      "Epoch 14/20\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 1.1386 - accuracy: 0.6018 - val_loss: 4.6428 - val_accuracy: 0.1071\n",
      "Epoch 15/20\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.9007 - accuracy: 0.7321 - val_loss: 4.8157 - val_accuracy: 0.1071\n",
      "Epoch 16/20\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.7297 - accuracy: 0.8071 - val_loss: 5.1842 - val_accuracy: 0.1071\n",
      "Epoch 17/20\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 0.5356 - accuracy: 0.8571 - val_loss: 5.2548 - val_accuracy: 0.1071\n",
      "Epoch 18/20\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.3930 - accuracy: 0.9071 - val_loss: 5.4160 - val_accuracy: 0.1000\n",
      "Epoch 19/20\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.3936 - accuracy: 0.8821 - val_loss: 5.8613 - val_accuracy: 0.1214\n",
      "Epoch 20/20\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.3682 - accuracy: 0.8786 - val_loss: 5.8900 - val_accuracy: 0.1286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17ec6e91610>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_seq_padded, y_train, validation_data=(X_test_seq_padded, y_test), epochs=20, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 10ms/step - loss: 5.8900 - accuracy: 0.1286\n",
      "Test loss: 5.889999866485596\n",
      "Test accuracy: 0.12857143580913544\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_seq_padded, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('edited_data.csv')\n",
    "max_length = max([len(verse.split()) for verse in df['Shloka']])\n",
    "num_classes = df['Chapter No'].nunique()\n",
    "X = df['Shloka']\n",
    "y = pd.get_dummies(df['Chapter No']).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings\n",
    "word_vectors = {}\n",
    "with open('glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        word_vectors[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_vectors:\n",
    "        embedding_matrix[i] = word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9/9 [==============================] - 4s 89ms/step - loss: 2.8865 - accuracy: 0.0911 - val_loss: 2.8732 - val_accuracy: 0.1286\n",
      "Epoch 2/30\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 2.8552 - accuracy: 0.1071 - val_loss: 2.8124 - val_accuracy: 0.1286\n",
      "Epoch 3/30\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 2.8189 - accuracy: 0.0893 - val_loss: 2.8245 - val_accuracy: 0.1286\n",
      "Epoch 4/30\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 2.8170 - accuracy: 0.1232 - val_loss: 2.8322 - val_accuracy: 0.1286\n",
      "Epoch 5/30\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 2.8193 - accuracy: 0.1071 - val_loss: 2.8250 - val_accuracy: 0.1286\n",
      "Epoch 6/30\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 2.8175 - accuracy: 0.1071 - val_loss: 2.8192 - val_accuracy: 0.1286\n",
      "Epoch 7/30\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.8189 - accuracy: 0.1089 - val_loss: 2.8324 - val_accuracy: 0.1286\n",
      "Epoch 8/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.8221 - accuracy: 0.1089 - val_loss: 2.8330 - val_accuracy: 0.1286\n",
      "Epoch 9/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.8214 - accuracy: 0.1089 - val_loss: 2.8398 - val_accuracy: 0.1286\n",
      "Epoch 10/30\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 2.8180 - accuracy: 0.0821 - val_loss: 2.8381 - val_accuracy: 0.1286\n",
      "Epoch 11/30\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 2.8117 - accuracy: 0.1107 - val_loss: 2.8212 - val_accuracy: 0.1286\n",
      "Epoch 12/30\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.8134 - accuracy: 0.1089 - val_loss: 2.8256 - val_accuracy: 0.1286\n",
      "Epoch 13/30\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.8114 - accuracy: 0.1089 - val_loss: 2.8298 - val_accuracy: 0.1286\n",
      "Epoch 14/30\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.8177 - accuracy: 0.1125 - val_loss: 2.8343 - val_accuracy: 0.1286\n",
      "Epoch 15/30\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 2.8105 - accuracy: 0.1089 - val_loss: 2.8397 - val_accuracy: 0.1286\n",
      "Epoch 16/30\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 2.8139 - accuracy: 0.1107 - val_loss: 2.8382 - val_accuracy: 0.1286\n",
      "Epoch 17/30\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 2.8059 - accuracy: 0.1161 - val_loss: 2.8321 - val_accuracy: 0.1286\n",
      "Epoch 18/30\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 2.8057 - accuracy: 0.1161 - val_loss: 2.8414 - val_accuracy: 0.1286\n",
      "Epoch 19/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.8030 - accuracy: 0.1125 - val_loss: 2.8446 - val_accuracy: 0.1286\n",
      "Epoch 20/30\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 2.8000 - accuracy: 0.1161 - val_loss: 2.8431 - val_accuracy: 0.1286\n",
      "Epoch 21/30\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.7979 - accuracy: 0.1125 - val_loss: 2.8484 - val_accuracy: 0.1286\n",
      "Epoch 22/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.7948 - accuracy: 0.1161 - val_loss: 2.8515 - val_accuracy: 0.1286\n",
      "Epoch 23/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.7944 - accuracy: 0.1161 - val_loss: 2.8587 - val_accuracy: 0.1286\n",
      "Epoch 24/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.7970 - accuracy: 0.1161 - val_loss: 2.8644 - val_accuracy: 0.1286\n",
      "Epoch 25/30\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.7985 - accuracy: 0.1089 - val_loss: 2.8630 - val_accuracy: 0.1286\n",
      "Epoch 26/30\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 2.7962 - accuracy: 0.1179 - val_loss: 2.8658 - val_accuracy: 0.1286\n",
      "Epoch 27/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.7933 - accuracy: 0.1161 - val_loss: 2.8664 - val_accuracy: 0.1286\n",
      "Epoch 28/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.7901 - accuracy: 0.1179 - val_loss: 2.8578 - val_accuracy: 0.1286\n",
      "Epoch 29/30\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.7895 - accuracy: 0.1179 - val_loss: 2.8538 - val_accuracy: 0.1286\n",
      "Epoch 30/30\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.7862 - accuracy: 0.1179 - val_loss: 2.8606 - val_accuracy: 0.1286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17ef23b4210>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train_seq_padded, y_train, validation_data=(X_test_seq_padded, y_test), epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 10ms/step - loss: 2.8606 - accuracy: 0.1286\n",
      "Test loss: 2.860588312149048\n",
      "Test accuracy: 0.12857143580913544\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_seq_padded, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the embeddings\n",
    "word_vectors = {}\n",
    "with open('glove/glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        word_vectors[word] = vector\n",
    "\n",
    "# Tokenize the input data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_vectors:\n",
    "        embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_seq_padded, y_train, validation_data=(X_test_seq_padded, y_test), epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_seq_padded, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained GloVe embeddings\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip -d glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
