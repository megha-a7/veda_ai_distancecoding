{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries: This code imports the libraries needed for the script, including pandas, numpy, re, string, nltk, sklearn, transformers, and torch.\n",
    "\n",
    "Define the model and tokenizer: This code defines the GPT-2 language model and tokenizer from the transformers library.\n",
    "\n",
    "Load and preprocess the data: This code loads and preprocesses the training data from a CSV file. The data is first filtered to keep only the Chapter No, Verse No, and Explanation columns. The text column is then created by combining these three columns and cleaned by converting all text to lowercase, removing digits, removing punctuation, and removing stop words.\n",
    "\n",
    "Load the questions and answers from the JSON file: This code loads the prompts and completions from a JSON file.\n",
    "\n",
    "Combine the prompts and completions to form the training data: This code combines the prompts and completions to form the input-output pairs for training the language model.\n",
    "\n",
    "Tokenize the inputs and outputs: This code tokenizes the input-output pairs using the GPT-2 tokenizer, pads them to the same length, and converts them to PyTorch tensors.\n",
    "\n",
    "Set up the learning rate schedule: This code sets up the learning rate schedule for training the language model using the AdamW optimizer and a linear schedule with warmup.\n",
    "\n",
    "Set up the device: This code sets up the device to use GPU if available, otherwise CPU.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VICTUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and tokenizer\n",
    "model_name = 'gpt2-medium'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "df = pd.read_csv('edited_data.csv')\n",
    "df = df[['Chapter No', 'Verse No', 'Explanation']]\n",
    "df['text'] = df['Chapter No'].astype(str) + ' ' + df['Verse No'].astype(str) + ' ' + df['Explanation'].astype(str)\n",
    "df.drop(['Chapter No', 'Verse No', 'Explanation'], axis=1, inplace=True)\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the questions and answers from the json file\n",
    "with open('training_data.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Combine the prompts and completions to form the training data\n",
    "inputs = []\n",
    "outputs = []\n",
    "for item in data:\n",
    "    inputs.append(item['prompt'])\n",
    "    outputs.append(item['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the inputs and outputs\n",
    "max_length = 512\n",
    "inputs = tokenizer(inputs, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "outputs = tokenizer(outputs, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up the learning rate schedule\n",
    "batch_size = 16  # reduce batch size\n",
    "num_epochs = 10\n",
    "num_train_steps = len(inputs['input_ids']) // batch_size * num_epochs\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this revised version, we first calculate the number of batches based on the input size and batch size, using integer division and adding 1 for any remaining samples in the last batch. We then use start_idx and end_idx to index into the input and output tensors for each batch. This avoids the need to add 1 to i+batch_size in the original code, which could cause an index out of range error if the last batch had fewer samples than the batch size. Additionally, we calculate the end index using min() to avoid indexing past the end of the input tensor. Finally, we use more descriptive variable names for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 32\n",
    "#num_batches = (len(inputs['input_ids']) - 1) // batch_size + 1  # calculate number of batches\n",
    "\n",
    "#for i in range(num_batches):\n",
    "    #start_idx = i * batch_size\n",
    "    #end_idx = min(start_idx + batch_size, len(inputs['input_ids']))  # last batch may have fewer samples\n",
    "    #batch_inputs = inputs['input_ids'][start_idx:end_idx].to(device)\n",
    "    #batch_outputs = inputs['input_ids'][start_idx+1:end_idx+1].to(device)\n",
    "    #outputs = model(batch_inputs, attention_mask=inputs['attention_mask'][start_idx:end_idx], \n",
    "                    #labels=batch_outputs, output_attentions=True, output_hidden_states=True)\n",
    "    #loss = outputs[0]\n",
    "    #loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for i in range(0, len(inputs['input_ids']) - batch_size + 1, batch_size):\n",
    "    batch_inputs = inputs['input_ids'][i:i+batch_size].to(device)\n",
    "    batch_outputs = inputs['input_ids'][i+1:i+batch_size+1].to(device)\n",
    "    outputs = model(batch_inputs, attention_mask=inputs['attention_mask'][i:i+batch_size], labels=batch_outputs, output_attentions=True, output_hidden_states=True)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]), batch_size):\n\u001b[0;32m      5\u001b[0m     batch_inputs \u001b[39m=\u001b[39m inputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m][i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m----> 6\u001b[0m     batch_outputs \u001b[39m=\u001b[39m outputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m][i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m      8\u001b[0m     \u001b[39m# Adjust batch size of target tensor\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch_outputs) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(batch_inputs):\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:286\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    285\u001b[0m     inner_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m (k, v) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m--> 286\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_dict[k]\n\u001b[0;32m    287\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_tuple()[k]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(inputs['input_ids']), batch_size):\n",
    "        batch_inputs = inputs['input_ids'][i:i+batch_size]\n",
    "        batch_outputs = outputs['input_ids'][i:i+batch_size]\n",
    "        \n",
    "        # Adjust batch size of target tensor\n",
    "        if len(batch_outputs) != len(batch_inputs):\n",
    "            batch_outputs = batch_outputs[:len(batch_inputs)]\n",
    "\n",
    "        outputs = model(batch_inputs, attention_mask=inputs['attention_mask'][i:i+batch_size], labels=batch_outputs, output_attentions=True, output_hidden_states=True)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print('Epoch:', epoch+1, 'Loss:', epoch_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8e840bfde2c9dbf1d829a22db5c55a245aabb8fa1c429845aff4444f12ec020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
