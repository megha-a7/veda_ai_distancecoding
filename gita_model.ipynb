{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data preprocessing\n",
    "\n",
    "Load the edited_data.csv file containing all the chapters and verses including explanations\n",
    "Preprocess the data by cleaning, tokenizing and removing stopwords\n",
    "\n",
    "Step 2: Model training\n",
    "\n",
    "Load the json file containing the set of questions and answers\n",
    "Fine-tune a pre-trained language model such as GPT-3 on the dataset using transfer learning\n",
    "Evaluate the performance of the model using a validation set\n",
    "\n",
    "Step 3: Model testing\n",
    "\n",
    "Ask the user to input a question\n",
    "Use the fine-tuned model to generate an answer based on the input question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VICTUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter No</th>\n",
       "      <th>Verse No</th>\n",
       "      <th>Shloka</th>\n",
       "      <th>English Translation</th>\n",
       "      <th>Explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>धृतराष्ट्र उवाच | धर्मक्षेत्रे कुरुक्षेत्रे सम...</td>\n",
       "      <td>Dhritarastra said: O Sanjaya, what did my sons...</td>\n",
       "      <td>The two armies had gathered on the battlefield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>सञ्जय उवाच । दृष्ट्वा तु पाण्डवानीकं व्यूढं दु...</td>\n",
       "      <td>Sanjaya said: But then, seeing the army of the...</td>\n",
       "      <td>Sanjay understood Dhritarashtra’s concern, who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् । व...</td>\n",
       "      <td>O teacher, (please) see this vast army of the ...</td>\n",
       "      <td>Duryodhana asked Dronacharya to look at the sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>अत्र शूरा महेष्वासा भीमार्जुनसमा युधि | युयुधा...</td>\n",
       "      <td>There are in this army, heroes wielding great ...</td>\n",
       "      <td>Due to his anxiety, the Pandava army seemed mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>धृष्टकेतुश्चेकितान: काशिराजश्च वीर्यवान् | पुर...</td>\n",
       "      <td>Dhrstaketu, Cekitana, and the valiant king of ...</td>\n",
       "      <td>Due to his anxiety, the Pandava army seemed mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chapter No  Verse No                                             Shloka  \\\n",
       "0           1         1  धृतराष्ट्र उवाच | धर्मक्षेत्रे कुरुक्षेत्रे सम...   \n",
       "1           1         2  सञ्जय उवाच । दृष्ट्वा तु पाण्डवानीकं व्यूढं दु...   \n",
       "2           1         3  पश्यैतां पाण्डुपुत्राणामाचार्य महतीं चमूम् । व...   \n",
       "3           1         4  अत्र शूरा महेष्वासा भीमार्जुनसमा युधि | युयुधा...   \n",
       "4           1         5  धृष्टकेतुश्चेकितान: काशिराजश्च वीर्यवान् | पुर...   \n",
       "\n",
       "                                 English Translation  \\\n",
       "0  Dhritarastra said: O Sanjaya, what did my sons...   \n",
       "1  Sanjaya said: But then, seeing the army of the...   \n",
       "2  O teacher, (please) see this vast army of the ...   \n",
       "3  There are in this army, heroes wielding great ...   \n",
       "4  Dhrstaketu, Cekitana, and the valiant king of ...   \n",
       "\n",
       "                                         Explanation  \n",
       "0  The two armies had gathered on the battlefield...  \n",
       "1  Sanjay understood Dhritarashtra’s concern, who...  \n",
       "2  Duryodhana asked Dronacharya to look at the sk...  \n",
       "3  Due to his anxiety, the Pandava army seemed mu...  \n",
       "4  Due to his anxiety, the Pandava army seemed mu...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "df = pd.read_csv('edited_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[['Chapter No', 'Verse No', 'Explanation']]\n",
    "df['text'] = df['Chapter No'].astype(str) + ' ' + df['Verse No'].astype(str) + ' ' + df['Explanation'].astype(str)\n",
    "df.drop(['Chapter No', 'Verse No', 'Explanation'], axis=1, inplace=True)\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))\n",
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the questions and answers from the json file\n",
    "with open('training_data.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the prompts and completions to form the training data\n",
    "prompts = []\n",
    "completions = []\n",
    "for item in data:\n",
    "    prompts.append(item['prompt'])\n",
    "    completions.append(item['completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the model and optimizer\n",
    "model = GPT2LMHeadModel.from_pretrained(model)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up the learning rate schedule for the training process of the GPT-2 model. Here's what each line does:\n",
    "\n",
    "# Set up the learning rate schedule: This is a comment that describes what the code does.\n",
    "1. batch_size = 32: This sets the batch size for the training process. The batch size determines how many training examples are processed in each iteration of the training loop. In this case, each iteration will process 32 examples.\n",
    "\n",
    "2. num_epochs = 10: This sets the number of training epochs. An epoch is a complete pass through the entire training dataset. In this case, the model will be trained for 10 epochs.\n",
    "\n",
    "3. num_train_steps = len(training_inputs) // batch_size * num_epochs: This calculates the total number of training steps. Each training step processes one batch of training examples. The len(training_inputs) function returns the number of training examples. The // operator performs integer division to determine how many batches there are. Multiplying this by num_epochs gives the total number of training steps.\n",
    "\n",
    "4. scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps): This sets up the learning rate schedule using the get_linear_schedule_with_warmup function from the transformers library. The learning rate schedule adjusts the learning rate during training to help the model converge faster and more accurately. The optimizer argument is the optimizer that will be used during training (in this case, AdamW). The num_warmup_steps argument is the number of warmup steps, where the learning rate is gradually increased from 0 to its initial value. The num_training_steps argument is the total number of training steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the learning rate schedule\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "num_train_steps = len(training_inputs) // batch_size * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(training_inputs), batch_size):\n",
    "        batch_inputs = training_inputs[i:i+batch_size]\n",
    "        batch_outputs = training_outputs[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_inputs, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        outputs = tokenizer(batch_outputs, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "        loss, _, _ = model(inputs['input_ids'], attention_mask=inputs['attention_mask'], labels=outputs['input_ids'], output_attentions=True, output_hidden_states=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses the Hugging Face Transformers library to generate text completions for a list of input texts using the GPT-2 model. Here is a breakdown of what each section of the code does:\n",
    "\n",
    "1. Load the GPT-2 model and tokenizer using the GPT2Tokenizer and pipeline classes from the Transformers library.\n",
    "2. Set the maximum length of the generated text using the max_length variable.\n",
    "3. Generate text completions for each input text in the texts list by encoding the text using the tokenizer, generating a completion using the GPT-2 model, and decoding the completion using the tokenizer.\n",
    "4. Extract the input_ids from each completion and store them in the completion_ids list.\n",
    "\n",
    "In summary, the code takes a list of input texts, uses the GPT-2 model to generate text completions for each input text, and extracts the input_ids from the completions. The completion_ids list can then be used for downstream tasks such as fine-tuning the model or generating further text completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 tokenizer\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the maximum length of the input sequence\n",
    "#max_length = 1024\n",
    "\n",
    "# Tokenize the text and truncate to max_length\n",
    "#input_ids = []\n",
    "#for text in texts:\n",
    "    #encoded = tokenizer.encode(text, truncation=True, max_length=max_length)\n",
    "    #input_ids.append(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import pipeline, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "#model = \"gpt2\"\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model)\n",
    "#gpt2 = pipeline(\"text-generation\", model=model)\n",
    "\n",
    "# Set the maximum length of the generated text\n",
    "#max_length = 1024\n",
    "\n",
    "# Generate text completions for each input text\n",
    "#completions = []\n",
    "#for text in texts:\n",
    "    #encoded = tokenizer.encode(text, truncation=True, max_length=max_length)\n",
    "    #completion = gpt2(tokenizer.decode(encoded), max_length=max_length)[0]\n",
    "    #completions.append(completion)\n",
    "\n",
    "# Extract the input_ids from the completions\n",
    "#completion_ids = [c[\"input_ids\"] for c in completions]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, GPT2Tokenizer\n",
    "import random\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model)\n",
    "gpt2 = pipeline(\"text-generation\", model=model)\n",
    "\n",
    "# Set the maximum length of the generated text\n",
    "max_length = 1024\n",
    "\n",
    "# Preprocess the training data\n",
    "training_inputs = []\n",
    "training_outputs = []\n",
    "for prompt, completion in zip(prompts, completions):\n",
    "    # Tokenize the prompt and completion and truncate to max_length\n",
    "    encoded_prompt = tokenizer.encode(prompt, truncation=True, max_length=max_length)\n",
    "    encoded_completion = tokenizer.encode(completion, truncation=True, max_length=max_length)\n",
    "\n",
    "    # Combine the prompt and completion into a single sequence\n",
    "    input_ids = encoded_prompt + encoded_completion[1:]  # Remove the initial token <|endoftext|>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completion_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Split the data into training and validation sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_inputs, validation_inputs, train_labels, validation_labels \u001b[39m=\u001b[39m train_test_split(input_ids, completion_ids, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m      3\u001b[0m train_masks, validation_masks, _, _ \u001b[39m=\u001b[39m train_test_split(input_ids, attention_masks, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completion_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, completion_ids, random_state=42, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(input_ids, attention_masks, random_state=42, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m max_length \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m tokenized_prompts \u001b[39m=\u001b[39m tokenizer(prompts, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49mmax_length, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      5\u001b[0m tokenized_completions \u001b[39m=\u001b[39m tokenizer(completions, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39mmax_length, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Split the data into training and validation sets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2530\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2528\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2529\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2530\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[0;32m   2531\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2532\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2616\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2611\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2612\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2613\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2614\u001b[0m         )\n\u001b[0;32m   2615\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2616\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[0;32m   2617\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[0;32m   2618\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2619\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2620\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   2621\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2622\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2623\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   2624\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2625\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2626\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   2627\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   2628\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   2629\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   2630\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   2631\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   2632\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2633\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2634\u001b[0m     )\n\u001b[0;32m   2635\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2636\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2637\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2638\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2654\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2655\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2798\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2781\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2782\u001b[0m \u001b[39mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[0;32m   2783\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2794\u001b[0m \u001b[39m        details in `encode_plus`).\u001b[39;00m\n\u001b[0;32m   2795\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2797\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m-> 2798\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[0;32m   2799\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2800\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   2801\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2802\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2803\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2804\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2805\u001b[0m )\n\u001b[0;32m   2807\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2808\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2809\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2824\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2825\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\VICTUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2435\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[1;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2433\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[0;32m   2434\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m-> 2435\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2439\u001b[0m     )\n\u001b[0;32m   2441\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[0;32m   2442\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2443\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[0;32m   2444\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2447\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m   2448\u001b[0m ):\n",
      "\u001b[1;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "# Tokenize the prompts and completions\n",
    "max_length = 1024\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenized_prompts = tokenizer(prompts, truncation=True, max_length=max_length, padding=True)\n",
    "tokenized_completions = tokenizer(completions, truncation=True, max_length=max_length, padding=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(tokenized_prompts['input_ids'], tokenized_completions['input_ids'], test_size=0.1, random_state=42)\n",
    "train_masks, val_masks, _, _ = train_test_split(tokenized_prompts['attention_mask'], tokenized_completions['attention_mask'], test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above had a time issue. \n",
    "\n",
    "This code uses the Hugging Face Transformers library to tokenize a list of input texts using the Roberta tokenizer. Here's a breakdown of what each section of the code does:\n",
    "\n",
    "1. Load the Roberta tokenizer using the RobertaTokenizer class from the Transformers library.\n",
    "\n",
    "2. Set the maximum length of the input sequence using the max_length variable.\n",
    "\n",
    "3. Tokenize each text in the texts list using the tokenizer by encoding the text and truncating it to max_length.\n",
    "\n",
    "4. Append the encoded text to the input_ids list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load the Roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Set the maximum length of the input sequence\n",
    "max_length = 512\n",
    "\n",
    "# Tokenize the text and truncate to max_length\n",
    "input_ids = []\n",
    "for text in texts:\n",
    "    encoded = tokenizer.encode(text, truncation=True, max_length=max_length)\n",
    "    input_ids.append(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'completion_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Split the data into training and validation sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_inputs, validation_inputs, train_labels, validation_labels \u001b[39m=\u001b[39m train_test_split(input_ids, completion_ids, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m      3\u001b[0m train_masks, validation_masks, _, _ \u001b[39m=\u001b[39m train_test_split(input_ids, attention_masks, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, test_size\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'completion_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, completion_ids, random_state=42, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(input_ids, attention_masks, random_state=42, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, completion_ids['input_ids'], random_state=42, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(input_ids, completion_ids['attention_mask'], random_state=42, test_size=0.1)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the data to PyTorch tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "#Create a DataLoader for the training and validation sets\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "#Load the pre-trained GPT-2 model and fine-tune it on the training data\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 1000\n",
    "epsilon = 1e-8\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "print('Training epoch {}'.format(epoch+1))\n",
    "total_loss = 0\n",
    "model.train()\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "b_inputs = batch[0].to(device)\n",
    "b_masks = batch[1].to(device)\n",
    "b_labels = batch[2].to(device)\n",
    "model.zero_grad()\n",
    "outputs = model(b_inputs, attention_mask=b_masks, labels=b_labels)\n",
    "loss = outputs[0]\n",
    "total_loss += loss.item()\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "optimizer.step()\n",
    "scheduler.step()\n",
    "if (step+1) % 50 == 0:\n",
    "print('Epoch: {}, Batch: {}, Loss: {}'.format(epoch+1, step+1, total_loss/50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "for batch in validation_dataloader:\n",
    "    b_inputs = batch[0].to(device)\n",
    "    b_masks = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_inputs, attention_mask=b_masks, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "average_train_loss = total_loss / len(train_dataloader)\n",
    "average_eval_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "print('  Average training loss: {0:.2f}'.format(average_train_loss))\n",
    "print('  Average validation loss: {0:.2f}'.format(average_eval_loss))\n",
    "\n",
    "# Ask the user to input a question\n",
    "while True:\n",
    "    prompt = input(\"Ask a question about the Gita: \")\n",
    "\n",
    "    # Tokenize the prompt and generate a response\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True, return_tensors='pt')\n",
    "    prompt_tokens = prompt_tokens.to(device)\n",
    "\n",
    "    generated = model.generate(\n",
    "        prompt_tokens,\n",
    "        max_length=1000,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the response\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8e840bfde2c9dbf1d829a22db5c55a245aabb8fa1c429845aff4444f12ec020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
